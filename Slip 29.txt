Slip 29 :




Q.1.	Take iris flower dataset and reduce 4D data to 2D data using PCA. Then train the model and predict new flower with given measurements.


# Import necessary libraries import numpy as np
import pandas as pd

from sklearn import datasets

from sklearn.model_selection import train_test_split from sklearn.decomposition import PCA
from sklearn.svm import SVC

from sklearn.metrics import accuracy_score

from sklearn.preprocessing import StandardScaler


# Step 1: Load the Iris dataset iris = datasets.load_iris()
X = iris.data # Features: sepal length, sepal width, petal length, petal width y = iris.target # Labels: species of iris flowers
 
# Step 2: Standardize the features (important for PCA) scaler = StandardScaler()
X_scaled = scaler.fit_transform(X)


# Step 3: Apply PCA to reduce 4D data to 2D pca = PCA(n_components=2)
X_pca = pca.fit_transform(X_scaled)


# Step 4: Split the dataset into training and testing sets (80% training, 20% testing)

X_train, X_test, y_train, y_test = train_test_split(X_pca, y, test_size=0.2, random_state=42)


# Step 5: Train the SVM classifier on the reduced data svm = SVC(kernel='linear', random_state=42) svm.fit(X_train, y_train)

# Step 6: Predict the flower species on the test set y_pred = svm.predict(X_test)

# Step 7: Evaluate the accuracy of the model accuracy = accuracy_score(y_test, y_pred)
print(f"Accuracy of the SVM model with PCA-reduced data: {accuracy * 100:.2f}%")
 


# Step 8: Predict flower species for a new flower with given measurements

# Example new flower data (sepal length, sepal width, petal length, petal width) new_flower = np.array([[5.1, 3.5, 1.4, 0.2]])

# Standardize the new flower data new_flower_scaled = scaler.transform(new_flower)

# Apply PCA transformation to the new flower new_flower_pca = pca.transform(new_flower_scaled)

# Predict using the trained SVM model predicted_class = svm.predict(new_flower_pca)
predicted_class_name = iris.target_names[predicted_class][0]


print(f"Predicted flower species for the input data {new_flower[0]}:
{predicted_class_name}")




Q.2.	Use K-means clustering model and classify the employees into various income groups or clusters. Preprocess data if require (i.e. drop missing or null values). Use elbow method and Silhouette Score to find value of k.

# Importing necessary libraries import numpy as np
 
import pandas as pd
import matplotlib.pyplot as plt
from sklearn.cluster import KMeans
from sklearn.preprocessing import StandardScaler from sklearn.metrics import silhouette_score

# Step 1: Load the dataset
# Assuming the dataset has columns such as 'EmployeeID', 'Age', 'Income', etc. # Replace 'employee_data.csv' with the actual path to your dataset
df = pd.read_csv('employee_data.csv')

# Step 2: Preprocess the data (handle missing values)
# Drop rows with missing values or fill them (here we drop) df.dropna(inplace=True)

# Assuming we are clustering based on 'Income' and 'Age' # Select relevant columns
X = df[['Income', 'Age']].values

# Step 3: Standardize the data scaler = StandardScaler() X_scaled = scaler.fit_transform(X)

# Step 4: Use the Elbow method to find the optimal number of clusters
# The elbow method involves plotting the sum of squared distances for a range of k values
inertia = []
k_range = range(1, 11)

for k in k_range:
kmeans = KMeans(n_clusters=k, random_state=42) kmeans.fit(X_scaled) inertia.append(kmeans.inertia_)

# Plotting the Elbow curve plt.figure(figsize=(8, 6))
plt.plot(k_range, inertia, marker='o', linestyle='-', color='b') plt.title('Elbow Method to Find Optimal K') plt.xlabel('Number of Clusters (k)')
plt.ylabel('Inertia (Sum of Squared Distances)') plt.show()
 

# Step 5: Use Silhouette Score to evaluate the clustering quality for different k sil_scores = []
for k in k_range[1:]:
kmeans = KMeans(n_clusters=k, random_state=42) kmeans.fit(X_scaled)
score = silhouette_score(X_scaled, kmeans.labels_) sil_scores.append(score)

# Plotting the Silhouette Scores plt.figure(figsize=(8, 6))
plt.plot(k_range[1:], sil_scores, marker='o', linestyle='-', color='g') plt.title('Silhouette Score for Different K')
plt.xlabel('Number of Clusters (k)') plt.ylabel('Silhouette Score') plt.show()

# Step 6: Fit K-means with the chosen optimal number of clusters optimal_k = 3 # Chosen based on elbow and silhouette score kmeans = KMeans(n_clusters=optimal_k, random_state=42) kmeans.fit(X_scaled)

# Step 7: Add the cluster labels to the original dataframe df['Cluster'] = kmeans.labels_

# Step 8: Display the clusters and their characteristics print(f"Cluster Centers:\n{kmeans.cluster_centers_}") print(f"Cluster Distribution:\n{df['Cluster'].value_counts()}")

# Step 9: Visualizing the clusters plt.figure(figsize=(8, 6))
plt.scatter(X_scaled[:, 0], X_scaled[:, 1], c=df['Cluster'], cmap='viridis') plt.title('Employee Clusters Based on Income and Age') plt.xlabel('Income (Standardized)')
plt.ylabel('Age (Standardized)') plt.show()

# Step 10: Classify a new employee (Example: Income = 50000, Age = 30) new_employee = np.array([[50000, 30]])

# Standardize the new data
 
new_employee_scaled = scaler.transform(new_employee)

# Predict the cluster for the new employee new_cluster = kmeans.predict(new_employee_scaled)
print(f"The new employee belongs to Cluster: {new_cluster[0]}")
