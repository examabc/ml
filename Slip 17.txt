Slip 17 :


Q.1.	Implement Ensemble ML algorithm on Pima Indians Diabetes Database with bagging (random forest), boosting, voting and Stacking methods and display analysis accordingly. Compare result

# Import necessary libraries import numpy as np
import pandas as pd

import matplotlib.pyplot as plt

from sklearn.model_selection import train_test_split from sklearn.preprocessing import StandardScaler
from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier,
VotingClassifier, StackingClassifier

from sklearn.linear_model import LogisticRegression from sklearn.svm import SVC
 
from sklearn.metrics import accuracy_score from sklearn.datasets import load_iris
from sklearn.tree import DecisionTreeClassifier from sklearn.neighbors import KNeighborsClassifier

# Step 1: Load the Pima Indians Diabetes Dataset

url = "https://raw.githubusercontent.com/jbrownlee/Datasets/master/pima-indians- diabetes.data.csv"

columns = ['Pregnancies', 'Glucose', 'BloodPressure', 'SkinThickness', 'Insulin', 'BMI', 'DiabetesPedigreeFunction', 'Age', 'Outcome']

data = pd.read_csv(url, names=columns)


# Step 2: Split the data into features and target variable X = data.drop('Outcome', axis=1)
y = data['Outcome']


# Step 3: Split the data into training and testing sets

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)


# Step 4: Standardize the features (important for some models like SVM) scaler = StandardScaler()
X_train = scaler.fit_transform(X_train)
 
X_test = scaler.transform(X_test)


# Step 5: Bagging - Random Forest Classifier

rf = RandomForestClassifier(n_estimators=100, random_state=42) rf.fit(X_train, y_train)
rf_pred = rf.predict(X_test)

rf_accuracy = accuracy_score(y_test, rf_pred)


# Step 6: Boosting - AdaBoost Classifier

ab = AdaBoostClassifier(n_estimators=100, random_state=42) ab.fit(X_train, y_train)
ab_pred = ab.predict(X_test)

ab_accuracy = accuracy_score(y_test, ab_pred)


# Step 7: Voting - Hard Voting Classifier

voting_clf = VotingClassifier(estimators=[('rf', rf), ('ab', ab)], voting='hard') voting_clf.fit(X_train, y_train)
voting_pred = voting_clf.predict(X_test) voting_accuracy = accuracy_score(y_test, voting_pred)

# Step 8: Stacking - Stacking Classifier

estimators = [('rf', rf), ('ab', ab), ('knn', KNeighborsClassifier())]
 
stacking_clf = StackingClassifier(estimators=estimators, final_estimator=LogisticRegression())

stacking_clf.fit(X_train, y_train) stacking_pred = stacking_clf.predict(X_test)
stacking_accuracy = accuracy_score(y_test, stacking_pred)


# Step 9: Display Results

print(f"Random Forest Accuracy: {rf_accuracy:.4f}") print(f"AdaBoost Accuracy: {ab_accuracy:.4f}") print(f"Voting Classifier Accuracy: {voting_accuracy:.4f}") print(f"Stacking Classifier Accuracy: {stacking_accuracy:.4f}")

# Step 10: Visualization of Comparison

methods = ['Random Forest', 'AdaBoost', 'Voting', 'Stacking']

accuracies = [rf_accuracy, ab_accuracy, voting_accuracy, stacking_accuracy]


plt.figure(figsize=(10, 6))

plt.barh(methods, accuracies, color='skyblue') plt.xlabel('Accuracy')
plt.title('Comparison of Ensemble Methods on Pima Indians Diabetes Dataset') plt.show()
 
Q.2.	Write a python program to implement Multiple Linear Regression for a house price dataset.


# Import necessary libraries import pandas as pd
import numpy as np

import matplotlib.pyplot as plt

from sklearn.model_selection import train_test_split from sklearn.linear_model import LinearRegression
from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score from sklearn.preprocessing import StandardScaler

# Step 1: Load the Dataset (Example Dataset - Replace with your own dataset) # Assuming the dataset has columns 'Size', 'Bedrooms', 'Age', and 'Price'
# Here, 'Price' is the target variable.

url = 'https://raw.githubusercontent.com/jbrownlee/Datasets/master/housing.csv' column_names = ['Size', 'Bedrooms', 'Age', 'Price']
data = pd.read_csv(url, names=column_names)


# Step 2: Preprocess Data # Check for missing values
print("Missing Values:\n", data.isnull().sum())
 


# Split the data into features (X) and target (y)

X = data[['Size', 'Bedrooms', 'Age']] # Features (independent variables) y = data['Price'] # Target variable (dependent variable)

# Step 3: Split Data into Training and Test Sets

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)


# Step 4: Feature Scaling (if necessary) scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train) X_test_scaled = scaler.transform(X_test)

# Step 5: Create and Train the Multiple Linear Regression Model model = LinearRegression()
model.fit(X_train_scaled, y_train)


# Step 6: Make Predictions

y_pred = model.predict(X_test_scaled)


# Step 7: Evaluate the Model
 
mse = mean_squared_error(y_test, y_pred) mae = mean_absolute_error(y_test, y_pred) r2 = r2_score(y_test, y_pred)

print("Mean Squared Error (MSE):", mse) print("Mean Absolute Error (MAE):", mae) print("R-squared (RÂ²):", r2)

# Step 8: Visualizing the predictions vs actual prices plt.scatter(y_test, y_pred)
plt.plot([min(y_test), max(y_test)], [min(y_test), max(y_test)], color='red') # Line of perfect fit

plt.xlabel('Actual Prices') plt.ylabel('Predicted Prices') plt.title('Actual vs Predicted Prices') plt.show()


